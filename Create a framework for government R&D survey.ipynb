{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3 - Create a framework for government R&D survey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import necessary modules/libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "# to avoid warnings to make the document output more readable\n",
    "# suppress all warnings by using the warnings module and using the function ‘filterwarnings()’.\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write a function for getting the text data from a website url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getdata(url):\n",
    "    r = requests.get(url)\n",
    "    return r.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write a function for getting all links from one page and store them in a list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, in this function we will get all “a href” marked links. As mentioned, this could potentially lead to the scraping of other websites you do not want information from. We have to place some restraints on the function.\n",
    "\n",
    "Second thing is that we also want the href that don’t show the full HTML link but only a relative link and starts with a “/” to be included in the collection of links."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Third, we want to convert the link to a dictionary with the function dict.fromkeys() to prevent saving duplications of the same link and to speed up the link searching process. The result looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create empty dict\n",
    "dict_href_links = {}\n",
    "\n",
    "# aşağıdaki fonksiyonda ikinci parametreyi linkin kökünü sabit tutmak için koydum\n",
    "def get_links(website_link,start_link):\n",
    "    html_data = getdata(website_link)\n",
    "    soup = BeautifulSoup(html_data, \"html.parser\")\n",
    "    list_links = []\n",
    "    for link in soup.find_all(\"a\", href=True):\n",
    "        \n",
    "        # Append to list if new link contains original link\n",
    "        if str(link[\"href\"]).startswith((str(start_link))):\n",
    "            list_links.append(link[\"href\"])\n",
    "            \n",
    "        # Include all href that do not start with website link but with \"/\"\n",
    "        if str(link[\"href\"]).startswith(\"/\"):\n",
    "            if link[\"href\"] not in dict_href_links:\n",
    "                print(link[\"href\"])\n",
    "                dict_href_links[link[\"href\"]] = None\n",
    "                link_with_www = website_link + link[\"href\"][1:]\n",
    "                print(\"adjusted link =\", link_with_www)\n",
    "                list_links.append(link_with_www)\n",
    "                \n",
    "    # Convert list of links to dictionary and define keys as the links and the values as \"Not-checked\"\n",
    "    dict_links = dict.fromkeys(list_links, \"Not-checked\")\n",
    "    return dict_links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write a function that loops over all the subpages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a for loop to go through the subpages and use tqdm to obtain insight into the number of steps that have been completed and keep track of the remaining time to complete the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subpage_links(l):\n",
    "    for link in tqdm(l):\n",
    "        # If not crawled through this page start crawling and get links\n",
    "        if l[link] == \"Not-checked\":\n",
    "            # aşağıdaki satırdaki website parametresi bir aşağıdaki komutta verilecek parametredir ve linkin kök hali içindir.\n",
    "            dict_links_subpages = get_links(link,website) \n",
    "            # Change the dictionary value of the link to \"Checked\"\n",
    "            l[link] = \"Checked\"\n",
    "        else:\n",
    "            # Create an empty dictionary in case every link is checked\n",
    "            dict_links_subpages = {}\n",
    "        # Add new dictionary to old dictionary\n",
    "        l = {**dict_links_subpages, **l}\n",
    "    return l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the URL list and Create the loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start the loop we have to initialize some variables. We save the website we want to scrape in a variable and convert this variable into a single key dictionary that has the value “Not-checked”. We create a counter “counter” to count the number of “Not-checked” links and we create a second counter “counter2” to count the number of iterations. To communicate back to ourselves we create some print statements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the below code I used the try except statement. The try except statement can handle exceptions. Exceptions may happen when you run a program. Exceptions are errors that happen during execution of the program. Python won’t tell you about errors like syntax errors (grammar faults), instead it will abruptly stop. An abrupt exit is bad for both the end user and developer. Instead of an emergency halt, you can use a try except statement to properly deal with the problem. An emergency halt will happen if you do not properly handle exceptions. https://pythonbasics.org/try-except/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"url_list.txt\") as file_in:\n",
    "    lines = []\n",
    "    for line in file_in:\n",
    "        lines.append(line)\n",
    "website_list = [x.rstrip() for x in lines] # remove line breaks\n",
    "website_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add website WITH slash on end\n",
    "# website_list=[\"https://www.kastamonu.bel.tr/v2/\",\"https://www.ilkadim.bel.tr/\"]\n",
    "links={}\n",
    "# create dictionary of website\n",
    "for website in website_list:\n",
    "    dict_links = {website:\"Not-checked\"}\n",
    "    counter, counter2 = None, 0\n",
    "    while counter!=0:\n",
    "        try:\n",
    "            counter2 += 1\n",
    "            dict_links2 = get_subpage_links(dict_links)\n",
    "            # Count number of non-values and set counter to 0 if there are no values within the dictionary equal to the string \"Not-checked\"\n",
    "            # https://stackoverflow.com/questions/48371856/count-the-number-of-occurrences-of-a-certain-value-in-a-dictionary-in-python\n",
    "            counter = sum(value == \"Not-checked\" for value in dict_links2.values())\n",
    "            # Print some statements\n",
    "            print(\"\")\n",
    "            print(\"THIS IS LOOP ITERATION NUMBER\", counter2)\n",
    "            print(\"LENGTH OF DICTIONARY WITH LINKS =\", len(dict_links2))\n",
    "            print(\"NUMBER OF 'Not-checked' LINKS = \", counter)\n",
    "            print(\"\")\n",
    "            dict_links = dict_links2\n",
    "            # I set the cutoff frequency to 200. Otherwise it takes a lot of time and usually 200 links are sufficient.\n",
    "            if len(dict_links2)>=200:\n",
    "                break\n",
    "        except:\n",
    "            pass\n",
    "    links.update(dict_links)\n",
    "    # Save list in json file\n",
    "    a_file = open(\"data.json\", \"w\", encoding='utf-8')\n",
    "    json.dump(links, a_file, ensure_ascii=False, indent=4)\n",
    "    a_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the number of not-checked links decreases until it reaches zero and then the script is finished, which is exactly what we want."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "source: https://python.plainenglish.io/scraping-the-subpages-on-a-website-ea2d4e3db113"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import previously produced json data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "data = json.load(open('data.json', 'r', encoding='utf-8'))\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get links from imported json data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = [t for t in data]\n",
    "links[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the keyword list and Scrape the list of words from url's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"keyword_list.txt\") as file_in2:\n",
    "    lines2 = []\n",
    "    for line in file_in2:\n",
    "        lines2.append(line)\n",
    "word_list = [x.rstrip() for x in lines2] # remove line breaks\n",
    "word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#arge sözcüğünü de eklesem mi, riskli, normal metnin içinde geçebilir\n",
    "#word_list=['Ar-Ge','AR-GE','Araştırma ve Geliştirme','araştırma ve geliştirme','ARAŞTIRMA VE GELİŞTİRME','Arge']\n",
    "count=0\n",
    "d={}\n",
    "for url in links:\n",
    "    print(\"\\nWebsite currently being scraped:\", url)\n",
    "    try:\n",
    "        for word in word_list:\n",
    "            r = requests.get(url, allow_redirects=False)\n",
    "            r.encoding='utf-8'\n",
    "            soup = BeautifulSoup(r.content, 'html.parser')\n",
    "            results = soup.body.find_all(string=re.compile('.*{0}.*'.format(word)), recursive=True)\n",
    "            print ('Found the word \"{0}\" {1} times\\n'.format(word, len(results)))\n",
    "            count+=len(results)\n",
    "    except:\n",
    "        pass\n",
    "    d.update({url: count})\n",
    "    print ('Found the list {0} times\\n'.format(count))\n",
    "    b_file = open(\"data1.json\", \"w\", encoding='utf-8')\n",
    "    json.dump(d, b_file, ensure_ascii=False, indent=4)\n",
    "    b_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print the url's scraped and cumulative number of found words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = json.load(open('data1.json', 'r', encoding='utf-8'))\n",
    "data1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show only the links containing keywords with their frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=0\n",
    "df = pd.DataFrame(columns=['Link', 'Frequency'])\n",
    "for x, y in data1.items():\n",
    "    if y>a:\n",
    "        b=[]\n",
    "        b.append(x)\n",
    "        b.append(y-a)\n",
    "        temporary_df = pd.DataFrame([b], columns=['Link', 'Frequency'])\n",
    "        df = df.append(temporary_df, ignore_index=True)\n",
    "        a=y\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel(\"sil.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
